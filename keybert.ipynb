{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3873a2d3",
   "metadata": {},
   "source": [
    "# KeyBERT Tutorial\n",
    "\n",
    "### Derrick Luyen\n",
    "### ICS 691B Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2091f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "# %pip install keybert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b2c5a",
   "metadata": {},
   "source": [
    "## What is KeyBERT?\n",
    "\n",
    "The main idea behind KeyBERT is the idea of keyword extraction, where KeyBERT is a technique used to extract keywords from its given input. KeyBERT focuses on extracting the words that are most similar to content / meaning of the input document(s).\n",
    "\n",
    "### Quick Explanation of BERT\n",
    "\n",
    "BERT, otherwise known as Bidirectional Encoder Representations from Transformers, is a machine learning technique that is used for natural language processing (NLP). The main focus of BERT is to help computers understand meanings of words that may have multiple meanings by using context clues from the words around it. \n",
    "\n",
    "KeyBERT takes advantage of BERT embeddings to get the key words / phrases, where BERT embeddings are vectors that are typically a length of 768 that are used to encode words. In simpler terms, BERT embeddings are vectors that represent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d97bf",
   "metadata": {},
   "source": [
    "## How does KeyBERT work?\n",
    "\n",
    "<img src=\"./images/keybert.png\" alt=\"keybert flow\" />\n",
    "\n",
    "### Input\n",
    "\n",
    "First of all, KeyBERT takes in a document or documents of text as input.\n",
    "\n",
    "### Tokenize Words / Phrases\n",
    "\n",
    "The first thing we need to do with this input document is to determine possible words and phrases to be extracted from the given input document, and KeyBERT uses CountVectorizer from the scikit-learn package to do so. What CountVectorizer does is that it creates tokens given a text input and a n-gram, where a n-gram allows us to choose a range of how many words we want in our key words / phrases, so if we choose something like (1,3) for example, we are telling KeyBERT and CountVectorizer to look for possible key words / phrases that contain between 1 and 3 words.\n",
    "\n",
    "### Extract Embeddings\n",
    "\n",
    "From there, we first use BERT to extract document embeddings, which is where we take the entire document, and we create an embedding for it, so our result is a vector that represents the entire document in the form of a vector. We then go after our word embeddings, where we take each word / phrase of the document and generate an embedding for each of them, so that each word / phrase then has its own vector to represent itself. How many words can be in the word / phrase is to be set later on in the method, but we just need to know that we can take advantage of extracting not only key words, but also key phrases (multiple words). \n",
    "\n",
    "How KeyBERT takes care of this step is through a package called Sentence Transformers. This package is one that can be used to compute vectors to represent the meanings of the words and the documents, and these vectors are the embeddings that are generated. The main benefit to KeyBERT using the Sentence Transformers library is that this library allows for the computation of vectors that are able to represent the meaning of complete documents, whereas something like Word2Vec can only be used for single words. \n",
    "\n",
    "### Comparisons\n",
    "\n",
    "Finally, after all of that, we then take the cosine similarity between the embeddings of each word / phrase and the original document embeddings, and from there, we can conclude on what words / phrases are the most similar to the input document(s) based on cosine similarity.\n",
    "\n",
    "### Model Note\n",
    "\n",
    "By default, KeyBERT uses the all-MiniLM-L6-v2 model if no other model is passed to the initializer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ad97b",
   "metadata": {},
   "source": [
    "### Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bf8977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised', 0.6676),\n",
       " ('labeled', 0.4896),\n",
       " ('learning', 0.4813),\n",
       " ('training', 0.4134),\n",
       " ('labels', 0.3947)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "         the learning algorithm to generalize from the training data to unseen situations in a\n",
    "         'reasonable' way (see inductive bias).\n",
    "      \"\"\"\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f135c92",
   "metadata": {},
   "source": [
    "### Explanation of Values\n",
    "\n",
    "We see that with the most basic call to extract_keywords, we are given 5 words, each with a value associated with it. That value is the calculated cosine similarity of the word and represents how similar the word is to the original input document. As mentioned above, this cosine similarity is a comparision between the word / phrase embedding vector and the input document vector. With cosine similarity, we know the highest value is 1 while the lowest is -1 as cosine fluctuates between those two values, and in this case, higher values means higher similarity between the word embedding and the document embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bb11f",
   "metadata": {},
   "source": [
    "## KeyBERT Methods\n",
    "\n",
    "The KeyBERT API consists of two main methods, which are extract_embeddings(...) and extract_keywords(...). The names of the methods are pretty self-explanatory, but extract_embeddings is used to extract the document embedding for the original document as well as the word embeddings for all of the key words / phrases.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6338e7",
   "metadata": {},
   "source": [
    "## Extract Embeddings\n",
    "\n",
    "The extract embeddings method is used to extract document and word embeddings. The document embedding is the embedding of the input document, and the word embeddings represent the embeddings of key words / phrases. This method returns two items, doc_embeddings and word embeddings, where doc_embeddings is the document embeddings for each input document, and word_embeddings is the embeddings of POTENTIAL key words / phrases from the given input document(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee52fec",
   "metadata": {},
   "source": [
    "### Extract Embeddings Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5939ad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.59579635e-02 -2.62582451e-02 -5.84359877e-02  2.30566636e-02\n",
      "   8.50326121e-02  4.17129025e-02  3.69997546e-02 -6.58201650e-02\n",
      "  -3.87015156e-02 -2.95363809e-03 -3.30499262e-02 -1.44510521e-02\n",
      "   5.30893803e-02  4.53584604e-02 -3.71540338e-02  3.82542983e-02\n",
      "   8.76147598e-02 -8.54388159e-03 -2.05052681e-02 -1.00440502e-01\n",
      "   3.98336798e-02  2.59869397e-02 -4.42725830e-02  5.32478541e-02\n",
      "  -4.35705557e-02  6.08860105e-02  3.51422019e-02  1.28424191e-03\n",
      "  -8.47642217e-03 -3.32369134e-02  2.45928876e-02 -4.37021479e-02\n",
      "   1.95506550e-02 -2.32752077e-02 -7.13654310e-02  2.95184217e-02\n",
      "  -5.31128272e-02  8.29254314e-02  1.79729536e-02 -4.40264381e-02\n",
      "   6.71629189e-03  3.34282182e-02 -2.02634130e-02  7.17922486e-03\n",
      "   6.06902204e-02  7.60842115e-02  2.87512783e-02 -5.89286201e-02\n",
      "  -9.60301980e-02  4.31644022e-02 -8.03859383e-02 -2.43355948e-02\n",
      "  -5.58135509e-02  3.92430387e-02 -3.63611020e-02 -1.13825584e-02\n",
      "   4.60491255e-02 -5.44759743e-02 -3.21346484e-02  6.92857802e-02\n",
      "   4.05919328e-02 -9.25480053e-02 -1.20178405e-02  3.38483648e-03\n",
      "   2.47528721e-02 -5.10513857e-02 -4.71547060e-03 -2.81510092e-02\n",
      "  -3.29866931e-02 -6.24616668e-02  3.51051576e-02  7.11539313e-02\n",
      "  -1.22090895e-02  2.20728926e-02  6.58348054e-02 -1.85684394e-02\n",
      "   1.58891585e-02  2.05849968e-02  4.97750975e-02  5.14318992e-04\n",
      "  -1.07067814e-02  3.21278796e-02  4.89481958e-03  2.29609404e-02\n",
      "   1.33996457e-01 -2.03497428e-02 -1.01014338e-02 -3.23385256e-03\n",
      "   1.86223164e-03  8.68496448e-02 -7.31765404e-02 -6.37987331e-02\n",
      "  -1.04203500e-01 -2.45212279e-02 -8.75536054e-02 -4.32481542e-02\n",
      "  -8.14334676e-03 -3.20247076e-02  5.27848750e-02  8.10106769e-02\n",
      "  -8.81583169e-02  7.95038715e-02  1.89339509e-03 -3.78542244e-02\n",
      "  -2.51657311e-02  2.51893494e-02 -6.47991989e-03 -6.85287789e-02\n",
      "   1.20860375e-01 -9.23453420e-02 -2.68241484e-02 -1.74136050e-02\n",
      "  -9.78136808e-02  1.23559346e-03 -4.23848076e-04 -2.14971099e-02\n",
      "   3.10618859e-02  2.18983414e-03 -6.32215068e-02  6.35793135e-02\n",
      "  -4.79216762e-02 -2.89728325e-02  8.18772316e-02  9.14352387e-02\n",
      "  -5.43998033e-02 -2.87247244e-02 -8.59841555e-02  2.65788709e-34\n",
      "   3.02150683e-03 -9.91961285e-02 -9.10557155e-03 -3.61363031e-02\n",
      "   4.30554710e-02 -6.35391548e-02 -6.48405850e-02  5.46675082e-03\n",
      "   5.98950796e-02  6.43365234e-02  4.47356440e-02 -2.78135948e-02\n",
      "   6.63355216e-02  7.47369155e-02 -3.83799151e-03 -5.79445064e-03\n",
      "  -2.45200507e-02  7.99367875e-02 -3.06397881e-02 -8.20871294e-02\n",
      "  -3.09578702e-02  2.87072882e-02  1.71526372e-02 -5.26083596e-02\n",
      "   2.54840553e-02  4.20914739e-02 -1.62964850e-03  2.18193773e-02\n",
      "   5.64957000e-02  1.54433853e-03  1.23385331e-02 -4.35653608e-03\n",
      "  -2.95356698e-02  6.18090155e-04  3.92428003e-02  1.26664042e-02\n",
      "   6.51749406e-06  5.91429489e-05  4.37822454e-02  1.38785848e-02\n",
      "   5.59151219e-03 -3.53771150e-02  6.68058023e-02 -4.41111065e-02\n",
      "  -4.39193361e-02  9.06770583e-03 -3.73114198e-02 -6.50375113e-02\n",
      "  -4.61265892e-02 -2.42730621e-02 -5.37549891e-03 -7.38403052e-02\n",
      "   2.45618075e-02 -1.90240983e-02 -3.86961438e-02  1.17142089e-01\n",
      "   7.77896307e-03  2.05936152e-02 -2.40098126e-02 -1.19238021e-02\n",
      "   4.60626483e-02  1.21449847e-02 -1.38792461e-02  5.18240072e-02\n",
      "  -1.12661282e-02  1.82789210e-02  6.30898103e-02 -1.00815482e-02\n",
      "   1.29445106e-01  4.77541192e-03 -9.07414127e-03  5.93460016e-02\n",
      "  -2.56465487e-02 -1.66222882e-02  8.30108970e-02  1.94760282e-02\n",
      "  -3.45548126e-03 -6.24274202e-02 -1.51121514e-02  4.26034853e-02\n",
      "  -4.23447555e-03 -1.83500703e-02 -1.10866409e-02 -8.42537433e-02\n",
      "  -4.52811420e-02 -1.41582068e-03 -4.41226438e-02 -4.01220657e-02\n",
      "   1.09817917e-02  3.01299207e-02 -1.15602344e-01  3.61451767e-02\n",
      "  -6.48752302e-02  4.83352877e-02  6.03185222e-02 -2.33045906e-33\n",
      "  -3.79601195e-02  9.21446756e-02 -2.57957801e-02  3.20161059e-02\n",
      "  -1.81577504e-02  1.65850371e-02 -5.24726324e-02  6.81604743e-02\n",
      "  -4.56911623e-02  5.15552722e-02 -8.71625450e-03 -1.92747544e-02\n",
      "   2.26287474e-03  6.50706664e-02 -6.57982901e-02  7.29076192e-02\n",
      "  -5.71689494e-02 -3.55880670e-02 -3.40939239e-02  3.55007835e-02\n",
      "  -3.60611780e-03  1.18641928e-01 -1.28038786e-03  2.74649728e-02\n",
      "   1.86599270e-02  2.51976177e-02  1.13816224e-02  3.80895026e-02\n",
      "   3.32392640e-02  5.41304983e-02 -4.06520702e-02 -6.30437434e-02\n",
      "  -1.69787575e-02 -5.30074202e-02 -5.86850494e-02  4.01312895e-02\n",
      "   9.51897874e-02 -3.12486757e-02 -4.83359396e-02  7.67844841e-02\n",
      "  -1.81771033e-02  9.10663083e-02 -7.93389753e-02 -7.71361915e-03\n",
      "  -2.28784382e-02 -9.68439355e-02 -2.28941999e-02  4.52446342e-02\n",
      "   1.70851760e-02 -1.18820295e-02 -4.06755023e-02  5.60234487e-02\n",
      "  -8.94420519e-02 -5.22594303e-02 -1.06289029e-01 -5.06013483e-02\n",
      "  -1.06266448e-02  2.33585425e-02  4.55362648e-02  3.91934142e-02\n",
      "  -5.00924215e-02 -3.30402777e-02 -1.02820592e-02  5.88442534e-02\n",
      "  -1.86757476e-03  1.14445761e-02 -1.87118873e-02  4.39912453e-02\n",
      "   6.71923533e-02 -4.13008686e-03  6.07469194e-02  6.04384169e-02\n",
      "  -3.79638560e-02  3.21814679e-02 -2.10239403e-02 -4.88420725e-02\n",
      "  -9.30591114e-03 -8.37088823e-02 -4.84574251e-02 -1.29831154e-02\n",
      "   7.16884956e-02 -1.16679654e-01  2.72960998e-02  5.12492098e-02\n",
      "   4.02702168e-02  2.28133406e-02  6.67531863e-02 -1.91695858e-02\n",
      "   2.11915839e-02 -7.75681213e-02 -2.40363646e-03  7.52693638e-02\n",
      "  -7.30238110e-02  5.99632636e-02 -1.18858740e-01 -5.01356858e-08\n",
      "  -9.47455540e-02 -6.85152709e-02  3.27610783e-02 -9.00472701e-03\n",
      "   2.64590755e-02 -2.59177424e-02 -1.59127060e-02  1.98000018e-02\n",
      "  -1.82978883e-02 -8.48122612e-02  3.55534963e-02 -5.09894639e-03\n",
      "  -3.88690680e-02  2.39237510e-02  6.34049028e-02  1.60551071e-02\n",
      "   5.37269972e-02  5.94139807e-02  2.93655731e-02  6.10098690e-02\n",
      "   2.49586347e-02 -1.53672369e-02 -1.47974014e-03  5.51229231e-02\n",
      "   7.50003457e-02 -1.14695914e-01  3.15347873e-03  8.17444101e-02\n",
      "  -1.66899953e-02  4.12958153e-02 -4.46285680e-02  8.55951458e-02\n",
      "   7.14321434e-02 -1.13334246e-02  8.37561712e-02  1.44941494e-01\n",
      "   2.61440827e-03 -1.03512913e-01 -5.56268357e-02 -6.91423891e-03\n",
      "  -6.75634965e-02  1.15450367e-01 -6.29073083e-02 -3.91279943e-02\n",
      "  -1.70068396e-03  3.46208252e-02  8.06523934e-02 -7.27853030e-02\n",
      "  -5.62258288e-02 -3.53810191e-02  4.54546534e-04  1.92376617e-02\n",
      "   3.29284519e-02  1.22157428e-02  4.53196578e-02  7.00420979e-03\n",
      "   1.93624534e-02 -9.88141447e-02 -1.29424706e-02  7.20112696e-02\n",
      "   2.81029823e-03  8.76084194e-02  7.91995600e-02 -3.18247899e-02]]\n",
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(doc)\n",
    "print(doc_embeddings)\n",
    "print(doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da46863",
   "metadata": {},
   "source": [
    "As we can see, doc_embeddings represents the entire document embedding for the input document, and we can also see that it has a shape of (1, 384), where 1 represents the one input document, and 384 represents the length of the document embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35e1e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 384)\n"
     ]
    }
   ],
   "source": [
    "print(word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2c595",
   "metadata": {},
   "source": [
    "The above shape of the word embeddings object tells us that there are 50 potential keywords from the document above that have been extracted and converted into vector embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b0993",
   "metadata": {},
   "source": [
    "### Extract Embeddings -> Extract Keywords\n",
    "\n",
    "Doc_embeddings and word_embeddings can also be passed to extract_keywords as parameters, and this would be the same as if you only passed in the original document, but we have the option to pass in embeddings in case we decide to calculate them a different way than the original intended way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "820bd00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('supervised', 0.6676), ('labeled', 0.4896), ('learning', 0.4813), ('training', 0.4134), ('labels', 0.3947)]\n"
     ]
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(doc, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38ead2",
   "metadata": {},
   "source": [
    "As we can see, the keywords here are the same as above, so we can either get the document + word embeddings to pass to the extract keywords method, or we can simply use extract keywords, which will take care of the embeddings by itself on its way to extracting the keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7eaa00",
   "metadata": {},
   "source": [
    "## Extract Embeddings Parameter Notes\n",
    "\n",
    "Here is the method definition of the extract embeddings method:\n",
    "\n",
    "extract_embeddings(self, docs, candidates=None, keyphrase_ngram_range=(1, 1), stop_words='english', min_df=1, vectorizer=None)\n",
    "\n",
    "The docs parameter is a required parameter as it indicates what / where to extract keywords from, whereas all of the other parameters are optional and have different use cases. For the vectorizer parameter, if it is passed a CountVectorizer from sklearn.feature_extraction.text.CountVectorizer, then all other parameters besides docs are not used. The candidates parameter allows you to pass in a list of potential key words to use instead of extracting them from the document. This parameter allows you to limit the keywords to whatever is passed in, and the keywords from this list with the highest cosine similarity when compared to the input document will be extracted as the keywords of the input document. The keyphrase_ngram_range parameter allows you to set the minimum and maximum number of words for key words / phrases, so for example, if you set the range to (1,2), this means KeyBERT will only look for key words and phrases that contain between 1 to 2 words. The stop_words parameter allows you to provide a list of stop words, which are essentially words that you want to filter out. The min_df parameter allows you to set a minimum amount of times a word has to appear across all documents in order for it to be in consideration as a keyword.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d73c2f",
   "metadata": {},
   "source": [
    "## Extract Keywords\n",
    "\n",
    "The extract_keywords method is used to extract key words or key phrases, and you can pass in multiple documents at once as input. This method uses cosine similarity to find words / phrases with closest distance to the entire input document(s) based on the document embedding and word embedding comparisons. This method returns the top n keywords with the closest cosine similarity to input document, where n is set to a default value of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e1203",
   "metadata": {},
   "source": [
    "A basic example of this extract_keyword method is shown above, so here are some examples of the parameters we can use to further customize and narrow down what we want from the method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1a5d2",
   "metadata": {},
   "source": [
    "## Extract Keywords Parameter Notes\n",
    "\n",
    "Here is the method definition of the extract keywords method:\n",
    "\n",
    "extract_keywords(self, docs, candidates=None, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=5, min_df=1, use_maxsum=False, use_mmr=False, diversity=0.5, nr_candidates=20, vectorizer=None, highlight=False, seed_keywords=None, doc_embeddings=None, word_embeddings=None)\n",
    "\n",
    "The docs, candidates, keyphrase_ngram_range, stop_words, min_df, and vectorizer work the same way as in the extract_embeddings method, so please refer to the above text about the extract embeddings parameters. The top_n parameter allows you to set the number of keywords / phrases you want to return, so if you want the top 10 keywords, you would then set top_n equal to 10. The highlight parameter is a more visual feature where it allows you to print the document text and highlight the key words or phrases that are found as a result of extract keywords. The seed_keywords parameter is a list of words that you want the extracted keywords to be similar to, so if you want to guide words in the direction of medicine for example, you might pass in words such as medicine, tylenol, fever, and etc... \n",
    "\n",
    "As mentioned earlier, the extract keywords method also allows you to pass in document embeddings and word embeddings, and both of these should be generated by the extract_embeddings method.\n",
    "\n",
    "Now, the last 4 parameters are more unique in the sense that they involve a little more explanation, but to give a basic gist of it, the use_maxsum parameter is a boolean that decides whether or not to use Max Sum Distance for keyword / keyphrase selection, and the nr_candidates parameter decides the number of candidates to consider, should use_maxsum be set to true. On the other hand, the use_mmr parameter is a boolean that decides whether or not to use Maximal Marginal Relevance for keyword / keyphrase selection, where the diversity parameter controls the diversity of the results, and this is set between 0 and 1. Both of these techniques are used to diversify the results of key words / phrases so that we can get different key words / phrases instead of just variations of the same word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cdc75a",
   "metadata": {},
   "source": [
    "## Max Sum Distance\n",
    "\n",
    "Max Sum Distance tries to increase diversity in the pool of keywords that are extracted. It is essentially where we take the top_n param (the default value if not passed manually is 5), and then we pool together the 2 x top_n most similar words / phrases, and lets call this list A. We then take combinations from A that consist of top_n words each, and we calculate cosine similarity on each combination. In the end, we extract the combination of top_n words that are the least similar to each other based on cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7921a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inductive', 0.2577),\n",
       " ('bias', 0.2644),\n",
       " ('function', 0.2658),\n",
       " ('supervisory', 0.3297),\n",
       " ('labels', 0.3947)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_ms = kw_model.extract_keywords(doc, use_maxsum=True)\n",
    "\n",
    "keywords_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620a893",
   "metadata": {},
   "source": [
    "For this example, we can see that compared to above when we extracted the keywords without max sum, we can see that labels is in common, but the other 4 words are different, providing the diversity we wanted when we decided to use Max Sum Distance. These words were found by taking the top 10 most similar words (as top_n was 5 in this case since that is the default value), and then pooling together combinations of 5 words each. We then take the cosine similarity of each of those combinations and extract the one that results in the least similar words based on the cosine similarity value, which then gives us the result we see above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c607fe8",
   "metadata": {},
   "source": [
    "## Maximal Marginal Relevance\n",
    "\n",
    "Maximal Marginal Relevance also tries to maximize the diversity of keywords choosen and this technique tries to make sure keywords selected are not too similar to each other. The amount of diversity depends on diversity value passed in, and the default value is 0.5. MMR takes into account how similar potential key words and phrases are to key words and phrases that have already been selected. By doing so, it chooses not to extract keywords that are too similar to each other, resulting in more variety in the words, rather than words that are just variations of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d04d0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised', 0.6676),\n",
       " ('training', 0.4134),\n",
       " ('function', 0.2658),\n",
       " ('bias', 0.2644),\n",
       " ('inductive', 0.2577)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_mmr = kw_model.extract_keywords(doc, use_mmr=True)\n",
    "\n",
    "keywords_mmr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026bd62",
   "metadata": {},
   "source": [
    "In this example, we can see that compared to the very first example above where labeled and labels were both keywords, in this case, since we use MMR, we exclude those words as they are too similar with one another, resulting in a more diversity key word pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd47818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised', 0.6676),\n",
       " ('labeled', 0.4896),\n",
       " ('learning', 0.4813),\n",
       " ('training', 0.4134),\n",
       " ('supervisory', 0.3297)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_mmr_75 = kw_model.extract_keywords(doc, use_mmr=True, diversity=0.25)\n",
    "\n",
    "keywords_mmr_75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830a33c",
   "metadata": {},
   "source": [
    "If we change the value of diversity to something like 0.35 as the example above shows, we get less diverse keywords, and compared to the words in the previous example with the default diversity of 0.5, we can see from a meaning perspective that these words seem more similar to each other than the words from the above example do, which is a result of the decreased diversity. For example, we get words like supervised and supervisory in this example, which are just variations of the same word, making them more similar which again is a result of the decreased diversity value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4f87a",
   "metadata": {},
   "source": [
    "## Using KeyBERT to extract keywords from a real dataset\n",
    "\n",
    "The URL to the dataset I will be using for this tutorial is here: https://www.kaggle.com/datasets/anandhuh/covid-abstracts?resource=download. This dataset contains 10,000 research papers centering around COVID-19, where each entry contains a title, abstract, and the url of the paper. Below I will be using KeyBERT to extract keywords from this COVID-19 Research Paper dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff42d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "csvFile = pandas.read_csv('./data/covid_abstracts.csv')\n",
    "\n",
    "print(len(csvFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381f9c0",
   "metadata": {},
   "source": [
    "First we want to read in the data from the file, so to accomplish this, we can use the read_csv method from the pandas package to easily read in all the entries. We then want to make sure that all the entries have been stored, and since we know there are 10,000 entries in the dataset, we can take the length of the read-in data, which I called csvFile, and we can see that it also has a length of 10,000, meaning all the data has been read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8c8383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "abstracts = []\n",
    "for abstract in csvFile['abstract']:\n",
    "    abstracts.append(abstract)\n",
    "    \n",
    "print(len(abstracts)) # contains all abstracts of the 10,000 research papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba190698",
   "metadata": {},
   "source": [
    "In our dataset, as I mentioned above, each record contains a title, abstract, and a url of the paper. The url is irrelevant in this case, and between the title and the abstract, I believe the abstract would contain more overall information about what the paper is about, and also provide more variation and words to consider for the extraction of the keywords which is why I chose to focus on only the abstracts of the papers.  To do this, I extract all of the abstracts of each paper to put into an array, and then I want to check the length of the array to make sure it is still 10,000 to prove I extracted all of the abstracts, and as we can see from above, that is indeed the case, so abstract extraction was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11cdc617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid', 0.473),\n",
       " ('hospitalization', 0.3638),\n",
       " ('patients', 0.2883),\n",
       " ('diagnosed', 0.2421),\n",
       " ('dakota', 0.2278)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_covid = kw_model.extract_keywords(abstracts[0])\n",
    "\n",
    "keywords_covid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07aed5e",
   "metadata": {},
   "source": [
    "Here is an initial example where we extract the keywords from the first abstract. This tells us that this paper likely focuses on COVID-19 hospitalization and dealing with patients who have been diagnosed with COVID-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3517815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('covid', 0.473),\n",
       "  ('hospitalization', 0.3638),\n",
       "  ('patients', 0.2883),\n",
       "  ('diagnosed', 0.2421),\n",
       "  ('dakota', 0.2278)],\n",
       " [('coronavirus', 0.4481),\n",
       "  ('immunity', 0.3374),\n",
       "  ('covid', 0.3309),\n",
       "  ('immunosuppression', 0.3027),\n",
       "  ('immune', 0.2837)],\n",
       " [('oncology', 0.3354),\n",
       "  ('resilience', 0.2778),\n",
       "  ('covid', 0.2625),\n",
       "  ('pandemic', 0.2533),\n",
       "  ('distress', 0.2459)],\n",
       " [('coronavirus', 0.3736),\n",
       "  ('covid', 0.3381),\n",
       "  ('clustering', 0.3139),\n",
       "  ('classifiers', 0.2915),\n",
       "  ('classification', 0.2868)],\n",
       " [('nasopharyngeal', 0.4071),\n",
       "  ('cov', 0.3387),\n",
       "  ('nasal', 0.3347),\n",
       "  ('covid', 0.3325),\n",
       "  ('infections', 0.3122)],\n",
       " [('hiv', 0.3844),\n",
       "  ('opioid', 0.3425),\n",
       "  ('outpatient', 0.31),\n",
       "  ('buprenorphine', 0.2951),\n",
       "  ('comorbid', 0.2854)],\n",
       " [('pandemic', 0.3228),\n",
       "  ('internet', 0.3161),\n",
       "  ('surveys', 0.3038),\n",
       "  ('digital', 0.2913),\n",
       "  ('digitalization', 0.2844)],\n",
       " [('receptor', 0.3791),\n",
       "  ('ace2', 0.338),\n",
       "  ('mutations', 0.3016),\n",
       "  ('molecular', 0.299),\n",
       "  ('virus', 0.2677)],\n",
       " [('coronavirus', 0.423),\n",
       "  ('covid', 0.3726),\n",
       "  ('pneumonia', 0.3658),\n",
       "  ('biomarkers', 0.2784),\n",
       "  ('pandemic', 0.2714)],\n",
       " [('inhibitors', 0.4112),\n",
       "  ('proteolytic', 0.404),\n",
       "  ('proteinase', 0.385),\n",
       "  ('inhibitor', 0.3729),\n",
       "  ('merbromin', 0.3506)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_10 = abstracts[:10]\n",
    "keywords_10 = kw_model.extract_keywords(first_10)\n",
    "\n",
    "keywords_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b49a3b",
   "metadata": {},
   "source": [
    "Here is what happens when we take the first 10 abstracts and try to attain the keywords of each paper. As we can see, each paper has different keywords as expected, although most of them are more or less centered around COVID. This can be useful for extracting keywords of individual documents, but what if we want to get a general idea of what the whole set of papers is about in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955e662b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid', 0.473),\n",
       " ('hospitalized', 0.411),\n",
       " ('hospitalization', 0.3638),\n",
       " ('coronavirus', 0.3614),\n",
       " ('pandemic', 0.3234)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_10 = \"\"\n",
    "for abstract in first_10:\n",
    "    set_10 += abstract + \" \"\n",
    "\n",
    "keywords_set = kw_model.extract_keywords(set_10)\n",
    "\n",
    "keywords_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a8f27",
   "metadata": {},
   "source": [
    "In that case, we can take the abstracts and combine them together, essentially to form a mega abstract that contains all of the 10 abstracts from above, and from there, we can then run extract keywords on that combined string, which shows us that the main focuses of these papers are indeed about the COVID-19 pandemic and the hospitalizations that occurred as a result of it. This can be useful when going through a set of similar papers where you want to get an initial idea of what exactly these papers have in common without having to read through all of them at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cacf37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid', 0.473),\n",
       " ('hospitalization', 0.3638),\n",
       " ('comorbidities', 0.2695),\n",
       " ('dakota', 0.2278),\n",
       " ('faulkton', 0.1662)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_set_mmr = kw_model.extract_keywords(set_10, use_mmr=True)\n",
    "\n",
    "keywords_set_mmr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26a0ee",
   "metadata": {},
   "source": [
    "These are the keywords we get when using the Maximal Margin Relevance Algorithm, and as we can see, they are all words that are not so related to each other, whereas in the previous example, there was hospitalized and hospitalization that were very related to each other. This shows us that the diversity of the key words was indeed increased as words that were very similar to each other were removed as a result of the MMR algorithm being applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7766cac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dakota', 0.2278),\n",
       " ('outpatient', 0.2489),\n",
       " ('vaccination', 0.2608),\n",
       " ('comorbidities', 0.2695),\n",
       " ('coronavirus', 0.3614)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_set_ms = kw_model.extract_keywords(set_10, use_maxsum=True)\n",
    "\n",
    "keywords_set_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcbe1d",
   "metadata": {},
   "source": [
    "By using Max Sum Distance, we get the 5 (top_n was default set to 5) keywords that are the least similar to each other in terms of cosine similarity from an initial group of 2 x top_n (10) keywords where we took combinations consisting of 5 keywords each to find the least similar group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "849ac504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patients covid', 0.5816),\n",
       " ('covid 19', 0.5577),\n",
       " ('severity covid', 0.5415),\n",
       " ('2019 covid', 0.534),\n",
       " ('pandemic covid', 0.4889)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_set_range = kw_model.extract_keywords(set_10, keyphrase_ngram_range=(1,2))\n",
    "\n",
    "keywords_set_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85382ec5",
   "metadata": {},
   "source": [
    "Another use case for extract_keywords is to extract key phrases where we can set the range of words to choose from, and in the example above, we wants key phrases that are either 1 or 2 words. Since we do not set any diversification algorithms like MMR or Max Sum, we get key phrases that are still similar, but the main idea is that we can allow phrases instead of just key words by adjusting the keyphrase_ngram_range parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "282e00d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid', 0.473),\n",
       " ('hospitalized', 0.411),\n",
       " ('hospitalization', 0.3638),\n",
       " ('coronavirus', 0.3614),\n",
       " ('pandemic', 0.3234),\n",
       " ('clinic', 0.3019),\n",
       " ('patients', 0.2883),\n",
       " ('cov', 0.2788),\n",
       " ('comorbidities', 0.2695),\n",
       " ('vaccination', 0.2608),\n",
       " ('clinical', 0.2581),\n",
       " ('illnesses', 0.2517)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_set_12 = kw_model.extract_keywords(set_10, top_n=12)\n",
    "\n",
    "keywords_set_12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7ba30",
   "metadata": {},
   "source": [
    "In this example, we adjust the top_n parameter in the extract keywords method, thus allowing us to change the number of keywords that we want to return. The default value for this is 5, and in this case, I chose to set this value to 12, giving us 12 keywords. This adjustment can be useful when you want more keywords for a document, which can give you a better idea of what the document is about as opposed to if you had fewer keywords to go off of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be808022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('facility coronavirus', 0.4556),\n",
       " ('coronavirus pandemic', 0.4503),\n",
       " ('hospitalized participants', 0.4479),\n",
       " ('hospitalization rates', 0.411),\n",
       " ('hospitalized', 0.411)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_set_filter = kw_model.extract_keywords(set_10, stop_words=['covid', '19'], keyphrase_ngram_range=(1,2))\n",
    "\n",
    "keywords_set_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909180c1",
   "metadata": {},
   "source": [
    "We can also add in our own stop words, so in this example, I filtered out covid and 19, just to see what other key phrases we may have since in the previous example, we saw a lot of keyphrases revolving around these two terms. As a result, we get a bit more diversity in the keywords, although they are still relatively similar, but the stop words added are indeed filtered out, which can be useful when you have a general idea of what a document is about and you are looking for other keywords that may come up to refine your idea of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c930d3c",
   "metadata": {},
   "source": [
    "## KeyBERT Limitations\n",
    "\n",
    "While KeyBERT is a very nice tool for extracting keywords, like every other tool out there, it does have its limitations. One of its limitations is execution time, in the sense that the full process of going through and using the KeyBERT methods can take a while, especially if we have large documents of text. The reason why this is the case is due to the BERT models that KeyBERT takes as input. By default, KeyBERT uses the all-MiniLM-L6-v2 model, but any BERT model can be passed into the KeyBERT initializer to be used. These models typically are very large in size and take up a lot of computer resources, and even more so when the input documents are larger, so depending on the scale of the project, KeyBERT can be a good tool to use, but it can also limit you in some ways. \n",
    "\n",
    "Another limitation with KeyBERT is the consistency of the key words and phrases. What I mean by this is that when producing key phrases, KeyBERT can produce phrases that are similar to the contents of the document via cosine similarity, but sometimes the phrases themselves might not make the most sense. For example, take a look at this example from earlier on:\n",
    "\n",
    "[('facility coronavirus', 0.4556),\n",
    " ('coronavirus pandemic', 0.4503),\n",
    " ('hospitalized participants', 0.4479),\n",
    " ('hospitalization rates', 0.411),\n",
    " ('hospitalized', 0.411)]\n",
    " \n",
    "For the most part, these phrases make sense, but the first one, facility coronavirus, is not very coherent. It would make much more sense if the words were switched for example to make 'coronavirus facility'. There is nowhere in the process flow of KeyBERT that checks for coherence as it mostly just focuses on embedding vectors, so that could be something added in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c4504",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall, KeyBERT is a great library and has useful methods for keyword and keyphrase extraction. It allows users to pass in various BERT models based on whatever model they need to use. Using this model, KeyBERT is then able to be used to extract the document and word embeddings of the given input document(s), as well as able to extract keywords from said input document(s). Each method has its own useful parameters to customize the code so that users can control what exactly it is they want to extract. Specifically, extract keywords has a bunch of useful parameters like use_mmr and use_maxsum to increase keyword diversity. However, with that being said, there are still some limitations for KeyBERT such as execution time depending on input documents and the model chosen as well as coherency of key phrases selected. Overall though, I would say KeyBERT is a great tool for key word and key phrase extraction and it has many useful features for users to take advantage of as they push forward on their keyword extraction journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af372a26",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "1. [KeyBERT GitHub](https://maartengr.github.io/KeyBERT/index.html)\n",
    "2. [How to Extract Relevant Keywords with KeyBERT](https://towardsdatascience.com/how-to-extract-relevant-keywords-with-keybert-6e7b3cf889ae)\n",
    "3. [KeyBERT vs YAKE](https://github.com/MaartenGr/KeyBERT/issues/25)\n",
    "4. [Keyword Extraction With KeyBERT](https://www.vennify.ai/keybert-keyword-extraction/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
